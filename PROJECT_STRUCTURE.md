# Brain Tumor Classification Project - Complete File & Folder Structure

## Project Overview

This project implements a deep learning pipeline for brain tumor grade classification (LGG vs HGG) using 3D MRI volumes from the BraTS 2018 dataset. The project follows a modular architecture with clear separation between data processing, model training, evaluation, and ensemble methods.

---

## Complete Directory Tree

```
brain_tumor_project/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # Main project documentation
â”œâ”€â”€ ğŸ“„ PROJECT_OVERVIEW.md                # Detailed project overview
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md               # This file - complete structure guide
â”œâ”€â”€ ğŸ“„ .gitignore                         # Git ignore rules
â”œâ”€â”€ ğŸ“„ brain_tumor_project.code-workspace # VS Code workspace configuration
â”œâ”€â”€ ğŸ“„ VS_CODE_WORKSPACE_SETUP.md        # VS Code setup instructions
â”‚
â”œâ”€â”€ ğŸ“ data/                              # All data (raw and processed)
â”‚   â”œâ”€â”€ raw/                              # Original, unmodified BraTS 2018 data
â”‚   â”‚   â””â”€â”€ BraTS2018/
â”‚   â”‚       â”œâ”€â”€ HGG/                      # High-Grade Glioma (210 cases)
â”‚   â”‚       â””â”€â”€ LGG/                      # Low-Grade Glioma (75 cases)
â”‚   â”‚
â”‚   â”œâ”€â”€ brats2018/                        # Alternative data location (symlinks/aliases)
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                        # Stage-wise processed data
â”‚   â”‚   â”œâ”€â”€ stage_0_raw/                  # Raw data reference
â”‚   â”‚   â”œâ”€â”€ stage_1_n4/                   # N4 bias field correction outputs
â”‚   â”‚   â”œâ”€â”€ stage_2_zscore/               # Z-score normalization outputs
â”‚   â”‚   â”œâ”€â”€ stage_3_crop/                 # ROI cropping outputs
â”‚   â”‚   â”œâ”€â”€ stage_4_resize/               # Resized to 128x128x128 (used for training)
â”‚   â”‚   â”œâ”€â”€ stage_5_augmented/            # (Runtime-only: intentionally empty)
â”‚   â”‚   â””â”€â”€ stage_6_balanced/             # (Runtime-only: intentionally empty)
â”‚   â”‚
â”‚   â”œâ”€â”€ entropy/                          # Entropy-based slice selection metadata (MIL-only)
â”‚   â”‚   â””â”€â”€ <patient_id>_entropy.json     # Per-patient entropy scores and top-k slice indices
â”‚   â”‚
â”‚   â”œâ”€â”€ entropy_results/                  # Entropy analysis results and statistics
â”‚   â”‚
â”‚   â”œâ”€â”€ index/                            # Patient index files
â”‚   â”‚   â””â”€â”€ stage4_index.csv              # Complete index of all patients after Stage 4
â”‚   â”‚
â”‚   â”œâ”€â”€ kfold_splits/                     # K-Fold split definitions (alternative location)
â”‚   â”‚
â”‚   â””â”€â”€ README.md                         # Data directory documentation
â”‚
â”œâ”€â”€ ğŸ“ models/                            # Model architecture definitions
â”‚   â”œâ”€â”€ resnet50_3d_fast/                 # ResNet50-3D model package
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # Package initialization
â”‚   â”‚   â””â”€â”€ model.py                      # ResNet50-3D architecture implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ swin_unetr_encoder.py             # Swin UNETR encoder for classification
â”‚   â”œâ”€â”€ dual_stream_mil.py                # Dual-Stream Multiple Instance Learning model
â”‚   â””â”€â”€ __pycache__/                      # Python cache files
â”‚
â”œâ”€â”€ ğŸ“ preprocessing/                     # Preprocessing utilities
â”‚   â””â”€â”€ 01_n4_bias.py                     # N4 bias correction utility functions
â”‚
â”œâ”€â”€ ğŸ“ scripts/                           # All executable scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/                    # Data preprocessing pipeline scripts
â”‚   â”‚   â”œâ”€â”€ run_stage1_n4.py              # Stage 1: N4 bias field correction
â”‚   â”‚   â”œâ”€â”€ run_stage2_zscore.py          # Stage 2: Z-score normalization
â”‚   â”‚   â”œâ”€â”€ run_stage3_crop.py            # Stage 3: ROI cropping
â”‚   â”‚   â””â”€â”€ run_stage4_resize.py          # Stage 4: Resize to fixed volume
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                         # Model training scripts
â”‚   â”‚   â”œâ”€â”€ train_resnet50_3d.py          # Train ResNet50-3D model
â”‚   â”‚   â”œâ”€â”€ train_swin_unetr_3d.py        # Train Swin UNETR model
â”‚   â”‚   â”œâ”€â”€ train_mil.py                  # Train single-modality MIL model
â”‚   â”‚   â”œâ”€â”€ train_mil_multi_modal.py      # Train multi-modality MIL model
â”‚   â”‚   â”œâ”€â”€ train_dual_stream_mil.py      # Train dual-stream MIL model
â”‚   â”‚   â””â”€â”€ run_mil_kfold.py              # K-Fold cross-validation runner for MIL
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                       # Model evaluation scripts (empty, to be implemented)
â”‚   â”‚
â”‚   â”œâ”€â”€ ensemble/                         # Ensemble method scripts
â”‚   â”‚   â”œâ”€â”€ prepare_oof_predictions.py    # Prepare out-of-fold predictions
â”‚   â”‚   â”œâ”€â”€ verify_and_merge_oof.py       # Verify and merge OOF predictions
â”‚   â”‚   â”œâ”€â”€ verify_merged_oof_for_training.py  # Verify merged OOF for meta-learner
â”‚   â”‚   â”œâ”€â”€ train_meta_learner.py         # Train meta-learner (stacking)
â”‚   â”‚   â”œâ”€â”€ test_ensemble_on_new_patients.py  # Test ensemble on new patients
â”‚   â”‚   â””â”€â”€ generate_visualizations.py    # Generate ensemble visualizations
â”‚   â”‚
â”‚   â”œâ”€â”€ splits/                           # Data splitting scripts
â”‚   â”‚   â”œâ”€â”€ build_stage4_index.py         # Build patient index from Stage 4 outputs
â”‚   â”‚   â””â”€â”€ make_kfold_splits.py          # Generate K-Fold cross-validation splits
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                         # Analysis and visualization scripts
â”‚   â”‚   â”œâ”€â”€ run_entropy_analysis.py       # Compute entropy scores for slices
â”‚   â”‚   â”œâ”€â”€ run_entropy_for_fold.py       # Compute entropy for specific fold
â”‚   â”‚   â””â”€â”€ visualize_entropy.py          # Visualize entropy results
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                            # Script-specific utility functions
â”‚   â”‚   â”œâ”€â”€ aggregate_mil_results.py      # Aggregate MIL model results across folds
â”‚   â”‚   â””â”€â”€ fix_swinunetr_metrics.py      # Fix Swin UNETR metrics computation
â”‚   â”‚
â”‚   â””â”€â”€ README.md                         # Scripts directory documentation
â”‚
â”œâ”€â”€ ğŸ“ utils/                             # Utility functions and helper modules
â”‚   â”œâ”€â”€ augmentations_3d.py               # 3D geometric data augmentation (Stage 5)
â”‚   â”œâ”€â”€ augmentations_2d.py               # 2D augmentation for MIL slice processing
â”‚   â”œâ”€â”€ class_balancing.py                # Class balancing utilities (Stage 6)
â”‚   â”œâ”€â”€ dataset_3d.py                     # PyTorch Dataset for 3D volumes
â”‚   â”œâ”€â”€ dataset_3d_multi_modal.py         # PyTorch Dataset for multi-modal 3D volumes
â”‚   â”œâ”€â”€ mil_dataset.py                    # PyTorch Dataset for single-modality MIL
â”‚   â”œâ”€â”€ mil_dataset_multi_modal.py        # PyTorch Dataset for multi-modality MIL
â”‚   â”œâ”€â”€ dataset_mil.py                    # Alternative MIL dataset implementation
â”‚   â”œâ”€â”€ entropy_analysis.py               # Entropy computation for slice selection
â”‚   â”œâ”€â”€ ldam_loss.py                      # LDAM (Large Margin) loss function
â”‚   â””â”€â”€ __pycache__/                      # Python cache files
â”‚
â”œâ”€â”€ ğŸ“ configs/                           # Configuration files (YAML)
â”‚   â”œâ”€â”€ stage_1_n4.yaml                   # Stage 1 preprocessing configuration
â”‚   â”œâ”€â”€ stage_2_zscore.yaml               # Stage 2 preprocessing configuration
â”‚   â”œâ”€â”€ stage_3_crop.yaml                 # Stage 3 preprocessing configuration
â”‚   â”œâ”€â”€ stage_4_resize.yaml               # Stage 4 preprocessing configuration
â”‚   â””â”€â”€ README.md                         # Configuration files documentation
â”‚
â”œâ”€â”€ ğŸ“ splits/                            # K-Fold cross-validation split definitions
â”‚   â”œâ”€â”€ kfold_5fold_seed42.json           # K-Fold summary (5 folds, seed=42)
â”‚   â”œâ”€â”€ fold_0_train.csv                  # Training set for fold 0
â”‚   â”œâ”€â”€ fold_0_val.csv                    # Validation set for fold 0
â”‚   â”œâ”€â”€ fold_1_train.csv                  # Training set for fold 1
â”‚   â”œâ”€â”€ fold_1_val.csv                    # Validation set for fold 1
â”‚   â”œâ”€â”€ fold_2_train.csv                  # Training set for fold 2
â”‚   â”œâ”€â”€ fold_2_val.csv                    # Validation set for fold 2
â”‚   â”œâ”€â”€ fold_3_train.csv                  # Training set for fold 3
â”‚   â”œâ”€â”€ fold_3_val.csv                    # Validation set for fold 3
â”‚   â”œâ”€â”€ fold_4_train.csv                  # Training set for fold 4
â”‚   â””â”€â”€ fold_4_val.csv                    # Validation set for fold 4
â”‚
â”œâ”€â”€ ğŸ“ pretrained/                        # Pretrained model weights
â”‚   â”œâ”€â”€ medicalnet_resnet50_3d.pth        # MedicalNet pretrained ResNet50-3D weights
â”‚   â””â”€â”€ README.md                         # Pretrained models documentation
â”‚
â”œâ”€â”€ ğŸ“ ensemble/                          # Ensemble method outputs and models
â”‚   â”œâ”€â”€ models/                           # Trained meta-learner models
â”‚   â”œâ”€â”€ oof_predictions/                  # Out-of-fold predictions from base models
â”‚   â”œâ”€â”€ results/                          # Ensemble evaluation results
â”‚   â”œâ”€â”€ visualizations/                   # Ensemble visualizations
â”‚   â””â”€â”€ README.md                         # Ensemble documentation
â”‚
â”œâ”€â”€ ğŸ“ results/                           # Training results and evaluation outputs
â”‚   â”œâ”€â”€ ResNet50-3D/                      # ResNet50-3D experiment results
â”‚   â”‚   â””â”€â”€ runs/
â”‚   â”‚       â””â”€â”€ fold_X/                   # Per-fold results
â”‚   â”‚           â””â”€â”€ YYYYMMDD_HHMMSS/
â”‚   â”‚               â”œâ”€â”€ checkpoints/      # Model checkpoints
â”‚   â”‚               â”œâ”€â”€ metrics/          # Evaluation metrics (JSON)
â”‚   â”‚               â”œâ”€â”€ plots/            # Training curves, confusion matrices
â”‚   â”‚               â””â”€â”€ predictions/      # Prediction outputs
â”‚   â”‚
â”‚   â”œâ”€â”€ Swin_UNETR/                       # Swin UNETR experiment results
â”‚   â”œâ”€â”€ SwinUNETR-3D/                     # Alternative Swin UNETR results
â”‚   â”œâ”€â”€ MIL/                              # Single-modality MIL results
â”‚   â”‚   â””â”€â”€ runs/
â”‚   â”‚       â””â”€â”€ fold_X/                   # Per-fold results
â”‚   â”‚           â””â”€â”€ YYYYMMDD_HHMMSS/
â”‚   â”‚               â”œâ”€â”€ checkpoints/
â”‚   â”‚               â”œâ”€â”€ metrics/
â”‚   â”‚               â”œâ”€â”€ plots/
â”‚   â”‚               â””â”€â”€ predictions/
â”‚   â”‚
â”‚   â”œâ”€â”€ DualStreamMIL-3D/                 # Dual-Stream MIL results
â”‚   â””â”€â”€ entropy_visualization/            # Entropy analysis visualizations
â”‚
â”œâ”€â”€ ğŸ“ experiments/                       # Experiment tracking
â”‚   â”œâ”€â”€ resnet50_3d/                      # ResNet50-3D experiments
â”‚   â”œâ”€â”€ swin_unetr/                       # Swin UNETR experiments
â”‚   â”œâ”€â”€ mil/                              # MIL experiments
â”‚   â””â”€â”€ README.md                         # Experiments documentation
â”‚
â”œâ”€â”€ ğŸ“ logs/                              # Training and preprocessing logs
â”‚   â”œâ”€â”€ preprocessing/                    # Preprocessing stage logs
â”‚   â”‚   â””â”€â”€ stageX_*.log                  # Per-stage processing logs
â”‚   â”œâ”€â”€ training/                         # Training logs
â”‚   â”‚   â””â”€â”€ *.log                         # Training run logs
â”‚   â”œâ”€â”€ evaluation/                       # Evaluation logs
â”‚   â””â”€â”€ README.md                         # Logs directory documentation
â”‚
â”œâ”€â”€ ğŸ“ docs/                              # Project documentation
â”‚   â”œâ”€â”€ stage1_n4_preprocessing.md        # Stage 1 documentation
â”‚   â”œâ”€â”€ stage2_zscore_preprocessing.md    # Stage 2 documentation
â”‚   â”œâ”€â”€ stage3_crop_preprocessing.md      # Stage 3 documentation
â”‚   â”œâ”€â”€ stage4_resize_preprocessing.md    # Stage 4 documentation
â”‚   â”œâ”€â”€ stage5_augmentation.md            # Stage 5 documentation
â”‚   â”œâ”€â”€ stage6_class_balancing.md         # Stage 6 documentation
â”‚   â”œâ”€â”€ stage7_kfold.md                   # Stage 7 (K-Fold) documentation
â”‚   â”œâ”€â”€ stage_entropy_mil.md              # Entropy analysis documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ resnet50_3d_training.md           # ResNet50-3D training guide
â”‚   â”œâ”€â”€ resnet50_3d_optimizations.md      # ResNet50-3D optimizations
â”‚   â”œâ”€â”€ resnet50_3d_training_fixes.md     # ResNet50-3D bug fixes
â”‚   â”œâ”€â”€ resnet50_3d_multimodal.md         # ResNet50-3D multimodal usage
â”‚   â”‚
â”‚   â”œâ”€â”€ swin_unetr_classification_proposal.md  # Swin UNETR proposal
â”‚   â”‚
â”‚   â”œâ”€â”€ mil_training.md                   # MIL training guide
â”‚   â”œâ”€â”€ mil_slice_selection_analysis.md   # MIL slice selection analysis
â”‚   â”œâ”€â”€ mil_performance_analysis.md       # MIL performance analysis
â”‚   â”œâ”€â”€ mil_overfitting_analysis_and_solution.md  # MIL overfitting solutions
â”‚   â”œâ”€â”€ mil_anti_overfitting_implementation_summary.md  # MIL anti-overfitting
â”‚   â”œâ”€â”€ mil_optimal_solution_implementation.md  # MIL optimal solution
â”‚   â”œâ”€â”€ mil_final_training_guide.md       # MIL final training guide
â”‚   â”œâ”€â”€ dual_stream_mil_design.md         # Dual-Stream MIL design
â”‚   â”œâ”€â”€ dual_stream_mil_implementation_summary.md  # Dual-Stream MIL implementation
â”‚   â”œâ”€â”€ dual_stream_mil_loss_analysis.md  # Dual-Stream MIL loss analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ ensemble_stacking_methodology.md  # Ensemble stacking methodology
â”‚   â”œâ”€â”€ ensemble_stacking_methodology Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø¯Ù…Ø¬.md  # Ensemble (Arabic)
â”‚   â”œâ”€â”€ ensemble_implementation_plan.md   # Ensemble implementation plan
â”‚   â”œâ”€â”€ ensemble_implementation_summary.md  # Ensemble implementation summary
â”‚   â”œâ”€â”€ ensemble_training_readiness_report.md  # Ensemble training readiness
â”‚   â”œâ”€â”€ ensemble_meta_learner_training_report.md  # Meta-learner training report
â”‚   â”œâ”€â”€ ensemble_oof_verification_report.md  # OOF verification report
â”‚   â”œâ”€â”€ ensemble_test_patients_guide.md   # Ensemble testing guide
â”‚   â”œâ”€â”€ ensemble_test_script_fixes.md     # Ensemble test fixes
â”‚   â”œâ”€â”€ ensemble_visualizations_summary.md  # Ensemble visualizations
â”‚   â”‚
â”‚   â”œâ”€â”€ training_journey_summary Ù…Ù„Ø®Øµ Ø§ÙˆÙ„ Ù…ÙˆØ¯ÙŠÙ„.md  # Training journey (Arabic)
â”‚   â”œâ”€â”€ training_journey_summary_SwinUNETR-3DÙ…Ù„Ø®Øµ ØªØ§Ù†ÙŠ Ù…ÙˆØ¯ÙŠÙ„.md  # Swin UNETR journey
â”‚   â”œâ”€â”€ training_journey_summary_DualStreamMIL-3DÙ…Ù„Ø®Øµ Ø«Ø§Ù„Ø« Ù…ÙˆØ¯ÙŠÙ„.md  # Dual-Stream MIL journey
â”‚   â”‚
â”‚   â”œâ”€â”€ entropy_visualization.md          # Entropy visualization guide
â”‚   â”œâ”€â”€ medicalnet_integration.md         # MedicalNet integration guide
â”‚   â””â”€â”€ training_strategy_analysis.md     # Training strategy analysis
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                         # Jupyter notebooks (minimal, for exploration)
â”‚
â”œâ”€â”€ ğŸ“ test/                              # Test data and scripts
â”‚   â””â”€â”€ DATA_FOR_TEST/                    # Test patient data
â”‚
â””â”€â”€ ğŸ“ __pycache__/                       # Python cache files (if any in root)
```

---

## Directory Descriptions

### ğŸ—‚ï¸ Root Level Files

#### `README.md`
Main project documentation containing:
- Project overview and goals
- Dataset information (BraTS 2018)
- Model architectures (ResNet50-3D, Swin UNETR, MIL)
- Preprocessing pipeline stages
- Usage instructions
- Reproducibility notes

#### `PROJECT_OVERVIEW.md`
Comprehensive technical overview with:
- Detailed project structure
- Preprocessing phase descriptions
- Training strategy explanations
- Data handling workflows
- Model-specific documentation

#### `PROJECT_STRUCTURE.md`
This file - complete file and folder structure reference guide.

#### `.gitignore`
Git ignore rules excluding:
- Large data files (`data/raw/`, `data/processed/`)
- Logs and checkpoints
- Python cache files
- IDE configurations

---

### ğŸ“ `data/` - Data Directory

**Purpose**: Centralized location for all data (raw, processed, and metadata).

#### `data/raw/`
- **Contents**: Original BraTS 2018 dataset (never modified)
- **Structure**: Organized by tumor grade (HGG/LGG), then by patient ID
- **Format**: NIfTI files (.nii/.nii.gz) with 4 modalities (T1, T1CE, T2, FLAIR) + segmentation masks

#### `data/processed/`
- **Contents**: Stage-wise processed data outputs
- **Stage 1-4**: Disk-based preprocessing (persistent files)
  - `stage_1_n4/`: N4 bias field correction outputs
  - `stage_2_zscore/`: Z-score normalized volumes
  - `stage_3_crop/`: ROI-cropped volumes
  - `stage_4_resize/`: Final preprocessed volumes (128x128x128) - **used for training**
- **Stage 5-6**: Runtime-only stages (directories intentionally empty)
  - `stage_5_augmented/`: Placeholder (augmentation applied in-memory during training)
  - `stage_6_balanced/`: Placeholder (class balancing applied via sampling)

#### `data/entropy/`
- **Purpose**: Entropy-based slice informativeness metadata (MIL models only)
- **Format**: JSON files per patient containing entropy scores and top-k slice indices
- **Usage**: Used by MIL models to select most informative 2D slices from 3D volumes

#### `data/index/`
- **Contents**: Patient index files
- **File**: `stage4_index.csv` - Complete index of all patients after Stage 4 preprocessing
- **Columns**: patient_id, class, class_label, path_t1, path_t1ce, path_t2, path_flair

---

### ğŸ“ `models/` - Model Architectures

**Purpose**: Model architecture definitions implemented in PyTorch.

#### `models/resnet50_3d_fast/`
- **Purpose**: ResNet50-3D model package
- **File**: `model.py` - 3D ResNet50 implementation for full volume classification
- **Input**: Full 3D volumes (128x128x128)
- **Pretrained**: Can load MedicalNet pretrained weights

#### `models/swin_unetr_encoder.py`
- **Purpose**: Swin UNETR encoder adapted for classification
- **Input**: Full 3D volumes (128x128x128)
- **Architecture**: Transformer-based encoder with patch embedding

#### `models/dual_stream_mil.py`
- **Purpose**: Dual-Stream Multiple Instance Learning model
- **Features**: Separate encoders per modality (FLAIR, T1ce), attention-based aggregation, fusion at bag level
- **Input**: Top-k 2D slices per modality (selected via entropy)

---

### ğŸ“ `scripts/` - Executable Scripts

**Purpose**: All runnable scripts organized by functionality.

#### `scripts/preprocessing/`
Preprocessing pipeline scripts executed sequentially:
1. **`run_stage1_n4.py`**: N4 bias field correction using SimpleITK
2. **`run_stage2_zscore.py`**: Z-score normalization (brain voxels only)
3. **`run_stage3_crop.py`**: ROI cropping with bounding box computation
4. **`run_stage4_resize.py`**: Resize to fixed volume (128x128x128)

Each script:
- Reads configuration from `configs/`
- Supports parallel processing
- Generates manifest files for tracking
- Creates logs in `logs/preprocessing/`

#### `scripts/training/`
Model training scripts:
- **`train_resnet50_3d.py`**: Train ResNet50-3D model
- **`train_swin_unetr_3d.py`**: Train Swin UNETR model
- **`train_mil.py`**: Train single-modality MIL model
- **`train_mil_multi_modal.py`**: Train multi-modality MIL model
- **`train_dual_stream_mil.py`**: Train dual-stream MIL model
- **`run_mil_kfold.py`**: K-Fold cross-validation runner for MIL

#### `scripts/ensemble/`
Ensemble method implementation:
- **`prepare_oof_predictions.py`**: Prepare out-of-fold predictions from base models
- **`verify_and_merge_oof.py`**: Verify and merge OOF predictions across folds
- **`train_meta_learner.py`**: Train meta-learner (stacking) using OOF predictions
- **`test_ensemble_on_new_patients.py`**: Test ensemble on new patient data
- **`generate_visualizations.py`**: Generate ensemble performance visualizations

#### `scripts/splits/`
Data splitting utilities:
- **`build_stage4_index.py`**: Build comprehensive patient index from Stage 4 outputs
- **`make_kfold_splits.py`**: Generate stratified K-Fold splits (k=5, seed=42)

#### `scripts/analysis/`
Analysis and visualization:
- **`run_entropy_analysis.py`**: Compute entropy scores for slice selection
- **`run_entropy_for_fold.py`**: Compute entropy for specific fold
- **`visualize_entropy.py`**: Visualize entropy analysis results

---

### ğŸ“ `utils/` - Utility Modules

**Purpose**: Reusable utility functions used across the project.

#### Core Utilities:
- **`augmentations_3d.py`**: 3D geometric augmentation transforms (Stage 5)
  - Random rotation, flip, zoom, translation
  - Medical-safe augmentations using MONAI
  
- **`augmentations_2d.py`**: 2D augmentation for MIL slice processing

- **`class_balancing.py`**: Class balancing utilities (Stage 6)
  - WeightedRandomSampler implementation
  - Inverse frequency weighting

- **`entropy_analysis.py`**: Entropy computation for slice informativeness
  - Shannon entropy calculation per slice
  - Top-k slice selection

- **`ldam_loss.py`**: LDAM (Large Margin) loss function
  - Label-Distribution-Aware Margin loss
  - Deferred Re-Weighting (DRW) support

#### Dataset Classes:
- **`dataset_3d.py`**: PyTorch Dataset for 3D volume loading
- **`dataset_3d_multi_modal.py`**: PyTorch Dataset for multi-modal 3D volumes
- **`mil_dataset.py`**: PyTorch Dataset for single-modality MIL
- **`mil_dataset_multi_modal.py`**: PyTorch Dataset for multi-modality MIL

---

### ğŸ“ `configs/` - Configuration Files

**Purpose**: YAML configuration files for preprocessing stages.

- **`stage_1_n4.yaml`**: N4 bias correction parameters
- **`stage_2_zscore.yaml`**: Z-score normalization parameters
- **`stage_3_crop.yaml`**: ROI cropping parameters (padding, bbox mode)
- **`stage_4_resize.yaml`**: Resize parameters (target size, interpolation)

All configs are human-readable YAML format for easy parameter tuning.

---

### ğŸ“ `splits/` - K-Fold Split Definitions

**Purpose**: Patient-level K-Fold cross-validation split definitions.

**Files**:
- **`kfold_5fold_seed42.json`**: Summary of K-Fold configuration
- **`fold_X_train.csv`**: Training set for fold X (X = 0-4)
- **`fold_X_val.csv`**: Validation set for fold X

**Features**:
- Stratified splitting (preserves class ratio)
- Patient-level (prevents data leakage)
- Reproducible (seed=42)

---

### ğŸ“ `pretrained/` - Pretrained Models

**Purpose**: Pretrained model weights for transfer learning.

- **`medicalnet_resnet50_3d.pth`**: MedicalNet pretrained ResNet50-3D weights
- Used for initializing ResNet50-3D models

---

### ğŸ“ `results/` - Training Results

**Purpose**: Training results, checkpoints, metrics, and visualizations.

**Structure** (per model type):
```
results/
â””â”€â”€ <ModelType>/
    â””â”€â”€ runs/
        â””â”€â”€ fold_X/
            â””â”€â”€ YYYYMMDD_HHMMSS/
                â”œâ”€â”€ checkpoints/      # Model checkpoints (best.pt, last.pt)
                â”œâ”€â”€ metrics/          # Evaluation metrics (JSON)
                â”œâ”€â”€ plots/            # Training curves, confusion matrices, ROC
                â”œâ”€â”€ predictions/      # Prediction outputs (numpy arrays)
                â””â”€â”€ logs/             # Training logs
```

**Model Types**:
- `ResNet50-3D/`
- `Swin_UNETR/` or `SwinUNETR-3D/`
- `MIL/`
- `DualStreamMIL-3D/`

---

### ğŸ“ `ensemble/` - Ensemble Outputs

**Purpose**: Ensemble method outputs and meta-learner models.

- **`models/`**: Trained meta-learner models
- **`oof_predictions/`**: Out-of-fold predictions from base models
- **`results/`**: Ensemble evaluation results
- **`visualizations/`**: Ensemble performance visualizations

---

### ğŸ“ `experiments/` - Experiment Tracking

**Purpose**: Experiment tracking and organization (optional structure for experiment management).

- `resnet50_3d/`: ResNet50-3D experiment outputs
- `swin_unetr/`: Swin UNETR experiment outputs
- `mil/`: MIL experiment outputs

---

### ğŸ“ `logs/` - Logs

**Purpose**: Training and preprocessing logs.

**Structure**:
- `preprocessing/`: Preprocessing stage logs (e.g., `stage1_n4_YYYYMMDD_HHMMSS.log`)
- `training/`: Training run logs
- `evaluation/`: Evaluation logs

---

### ğŸ“ `docs/` - Documentation

**Purpose**: Comprehensive project documentation organized by topic.

**Categories**:
- **Preprocessing**: Stage-by-stage preprocessing documentation
- **Model Training**: Training guides for each model type
- **MIL Models**: Extensive MIL documentation (overfitting, optimizations, etc.)
- **Ensemble**: Ensemble methodology and implementation guides
- **Training Journeys**: Summaries of training experiences (some in Arabic)
- **Technical Guides**: Entropy analysis, MedicalNet integration, etc.

---

## Key Workflows

### 1. Data Preprocessing Workflow

```
data/raw/BraTS2018/
  â†“ (Stage 1: N4)
data/processed/stage_1_n4/
  â†“ (Stage 2: Z-score)
data/processed/stage_2_zscore/
  â†“ (Stage 3: Crop)
data/processed/stage_3_crop/
  â†“ (Stage 4: Resize)
data/processed/stage_4_resize/  â† Used for training
```

### 2. Training Workflow

```
Stage 4 Data
  â†“
K-Fold Splits (splits/)
  â†“
Training Scripts (scripts/training/)
  â†“
Results (results/<ModelType>/)
```

### 3. MIL-Specific Workflow

```
Stage 4 Data
  â†“
Entropy Analysis (scripts/analysis/run_entropy_analysis.py)
  â†“
data/entropy/ (JSON files)
  â†“
MIL Training (scripts/training/train_mil*.py)
  â†“
Results (results/MIL/)
```

### 4. Ensemble Workflow

```
Base Model Results (results/)
  â†“
OOF Predictions (scripts/ensemble/prepare_oof_predictions.py)
  â†“
Merge OOF (scripts/ensemble/verify_and_merge_oof.py)
  â†“
Train Meta-Learner (scripts/ensemble/train_meta_learner.py)
  â†“
Test Ensemble (scripts/ensemble/test_ensemble_on_new_patients.py)
  â†“
Ensemble Results (ensemble/results/)
```

---

## File Naming Conventions

### Data Files
- **Raw data**: `<patient_id>_<modality>.nii` or `.nii.gz`
- **Processed data**: `<patient_id>_<modality>.nii.gz`
- **Entropy files**: `<patient_id>_entropy.json`

### Model Files
- **Checkpoints**: `best.pt`, `last.pt`
- **Pretrained**: `<model_name>.pth`

### Split Files
- **CSV splits**: `fold_X_train.csv`, `fold_X_val.csv`
- **JSON summary**: `kfold_<k>fold_seed<seed>.json`

### Log Files
- **Preprocessing**: `stageX_<description>_YYYYMMDD_HHMMSS.log`
- **Training**: `training_YYYYMMDD_HHMMSS.log`

### Result Files
- **Metrics**: `metrics.json`, `threshold_analysis.json`
- **Plots**: `training_curves.png`, `confusion_matrix.png`, `roc_curve.png`
- **Predictions**: `val_probs.npy`, `val_preds.npy`, `val_labels.npy`

---

## Important Notes

### Runtime-Only Stages
- **Stage 5 (Augmentation)** and **Stage 6 (Balancing)** do NOT create files on disk
- They are applied dynamically during DataLoader iteration
- Directories exist but are intentionally empty

### Data Persistence
- All data is stored on persistent volume (`/workspace/`) that survives pod restarts
- Raw data is NEVER modified (read-only)
- All processed outputs go to `data/processed/`

### Model Input Formats
- **ResNet50-3D & Swin UNETR**: Full 3D volumes (128x128x128)
- **MIL Models**: Top-k 2D slices (selected via entropy) per modality

### Reproducibility
- Fixed random seeds (seed=42 for K-Fold)
- Configuration files for all preprocessing stages
- Deterministic augmentation when applicable

---

## Getting Started

1. **Preprocessing**: Run scripts in `scripts/preprocessing/` sequentially (Stage 1 â†’ 4)
2. **Generate Splits**: Run `scripts/splits/build_stage4_index.py` then `scripts/splits/make_kfold_splits.py`
3. **Train Models**: Run appropriate training script from `scripts/training/`
4. **Evaluate**: Check results in `results/<ModelType>/`
5. **Ensemble** (optional): Follow ensemble workflow in `scripts/ensemble/`

For detailed instructions, see individual README files in each directory and documentation in `docs/`.

