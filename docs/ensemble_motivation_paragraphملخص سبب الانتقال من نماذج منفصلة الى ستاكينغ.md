# Motivation for Ensemble Learning

Three architecturally distinct models were implemented for the binary brain tumor classification task, each following a different learning paradigm: a ResNet50-based convolutional neural network leveraging 3D spatial convolutions and pretrained medical imaging weights, a Swin UNETR encoder-based transformer model utilizing window-based self-attention mechanisms to capture long-range dependencies, and a Dual-Stream Multiple Instance Learning model that aggregates instance-level representations into bag-level predictions through attention-based mechanisms. This architectural diversity is methodologically significant because each model inherently captures different characteristics of the input dataâ€”the convolutional approach emphasizes local spatial patterns and hierarchical feature extraction, the transformer architecture focuses on global contextual relationships and inter-region dependencies, while the MIL paradigm explicitly models the relationship between informative instances and patient-level labels. Ensemble learning theory suggests that combining such diverse models can reduce shared error patterns, as the distinct representational biases of each architecture lead to different failure modes and decision boundaries. When models make errors on different subsets of the data or for different reasons, their combination through appropriate fusion strategies can compensate for individual weaknesses while preserving their complementary strengths. Therefore, integrating these three models into an ensemble framework represents a natural methodological progression that leverages the inherent diversity of the implemented architectures.

