# Ensemble Stacking Methodology

The ensemble approach follows a two-level stacking framework where base model predictions serve as input features for a meta-learner that learns to optimally combine their outputs. This methodology is preferred over simple averaging or majority voting because it allows the meta-learner to learn the relative reliability and complementary strengths of each base model across different regions of the input space, rather than assuming equal contribution from all models. The stacking ensemble operates at the probability level, using the predicted probabilities for the HGG class from each of the three base models as input features, which preserves the uncertainty information inherent in each model's predictions and enables more nuanced fusion than hard-label voting. To prevent data leakage and ensure proper generalization, the meta-learner is trained exclusively on out-of-fold predictions generated during the 5-fold cross-validation process. Specifically, for each fold, the base models trained on the training set produce predictions on the corresponding validation set, and these validation predictions are aggregated across all five folds to form a complete set of out-of-fold predictions that serve as the meta-learner's training data. This approach ensures that the meta-learner never sees predictions on data that was used to train the base models, maintaining the integrity of the cross-validation protocol. A lightweight logistic regression model is employed as the meta-learner, chosen for its low capacity and reduced risk of overfitting given the small number of input features (three base model predictions) relative to the dataset size. This choice is further justified by the interpretability of linear combinations, which allows for straightforward analysis of how each base model contributes to the final ensemble prediction. During inference on unseen test data, the trained base models generate predictions that are then fed to the trained meta-learner to produce the final ensemble prediction, ensuring that the same methodology applied during cross-validation is consistently applied to new data.

